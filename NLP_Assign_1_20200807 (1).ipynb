{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUoJ7QfOIICp",
        "outputId": "6260af7b-ffb9-4db9-b522-a9879fb3a3d7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Extract HTML from URL\n",
        "url = \"https://en.wikipedia.org/wiki/Word2vec\"\n",
        "response = requests.get(url)\n",
        "html_content = response.text\n",
        "print(\"HTML Content before processing:\")\n",
        "print(html_content[:5000])\n",
        "\n",
        "#Extract TEXT from HTML page\n",
        "soup = BeautifulSoup(html_content, \"html.parser\")\n",
        "text = \" \".join([p.text for p in soup.find_all([\"p\", \"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\"])])  # Extract text from specific tags"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TT4cFahII_HH",
        "outputId": "6ca381a7-8698-49a3-9008-eb0ec0c44c05"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HTML Content before processing:\n",
            "<!DOCTYPE html>\n",
            "<html class=\"client-nojs vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-sticky-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pinned-clientpref-1 vector-feature-main-menu-pinned-disabled vector-feature-limited-width-clientpref-1 vector-feature-limited-width-content-enabled vector-feature-custom-font-size-clientpref-0 vector-feature-client-preferences-disabled vector-feature-client-prefs-pinned-disabled vector-feature-night-mode-disabled skin-night-mode-clientpref-0 vector-toc-available\" lang=\"en\" dir=\"ltr\">\n",
            "<head>\n",
            "<meta charset=\"UTF-8\">\n",
            "<title>Word2vec - Wikipedia</title>\n",
            "<script>(function(){var className=\"client-js vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-sticky-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pinned-clientpref-1 vector-feature-main-menu-pinned-disabled vector-feature-limited-width-clientpref-1 vector-feature-limited-width-content-enabled vector-feature-custom-font-size-clientpref-0 vector-feature-client-preferences-disabled vector-feature-client-prefs-pinned-disabled vector-feature-night-mode-disabled skin-night-mode-clientpref-0 vector-toc-available\";var cookie=document.cookie.match(/(?:^|; )enwikimwclientpreferences=([^;]+)/);if(cookie){cookie[1].split('%2C').forEach(function(pref){className=className.replace(new RegExp('(^| )'+pref.replace(/-clientpref-\\w+$|[^\\w-]+/g,'')+'-clientpref-\\\\w+( |$)'),'$1'+pref+'$2');});}document.documentElement.className=className;}());RLCONF={\"wgBreakFrames\":false,\"wgSeparatorTransformTable\":[\"\",\"\"],\n",
            "\"wgDigitTransformTable\":[\"\",\"\"],\"wgDefaultDateFormat\":\"dmy\",\"wgMonthNames\":[\"\",\"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\"July\",\"August\",\"September\",\"October\",\"November\",\"December\"],\"wgRequestId\":\"6a6648a3-e651-4415-be6b-f6cdd21fce10\",\"wgCanonicalNamespace\":\"\",\"wgCanonicalSpecialPageName\":false,\"wgNamespaceNumber\":0,\"wgPageName\":\"Word2vec\",\"wgTitle\":\"Word2vec\",\"wgCurRevisionId\":1212740028,\"wgRevisionId\":1212740028,\"wgArticleId\":47527969,\"wgIsArticle\":true,\"wgIsRedirect\":false,\"wgAction\":\"view\",\"wgUserName\":null,\"wgUserGroups\":[\"*\"],\"wgCategories\":[\"CS1 errors: missing periodical\",\"Articles with short description\",\"Short description is different from Wikidata\",\"Wikipedia articles needing clarification from February 2022\",\"Use dmy dates from April 2017\",\"Free science software\",\"Natural language processing toolkits\",\"Artificial neural networks\",\"Semantic relations\"],\"wgPageViewLanguage\":\"en\",\"wgPageContentLanguage\":\"en\",\"wgPageContentModel\":\"wikitext\",\"wgRelevantPageName\":\n",
            "\"Word2vec\",\"wgRelevantArticleId\":47527969,\"wgIsProbablyEditable\":true,\"wgRelevantPageIsProbablyEditable\":true,\"wgRestrictionEdit\":[],\"wgRestrictionMove\":[],\"wgNoticeProject\":\"wikipedia\",\"wgFlaggedRevsParams\":{\"tags\":{\"status\":{\"levels\":1}}},\"wgMediaViewerOnClick\":true,\"wgMediaViewerEnabledByDefault\":true,\"wgPopupsFlags\":6,\"wgVisualEditor\":{\"pageLanguageCode\":\"en\",\"pageLanguageDir\":\"ltr\",\"pageVariantFallbacks\":\"en\"},\"wgMFDisplayWikibaseDescriptions\":{\"search\":true,\"watchlist\":true,\"tagline\":false,\"nearby\":true},\"wgWMESchemaEditAttemptStepOversample\":false,\"wgWMEPageLength\":30000,\"wgULSCurrentAutonym\":\"English\",\"wgCentralAuthMobileDomain\":false,\"wgEditSubmitButtonLabelPublish\":true,\"wgULSPosition\":\"interlanguage\",\"wgULSisCompactLinksEnabled\":false,\"wgVector2022LanguageInHeader\":true,\"wgULSisLanguageSelectorEmpty\":false,\"wgWikibaseItemId\":\"Q22673982\",\"wgCheckUserClientHintsHeadersJsApi\":[\"architecture\",\"bitness\",\"brands\",\"fullVersionList\",\"mobile\",\"model\",\"platform\",\"platformVersion\"],\n",
            "\"GEHomepageSuggestedEditsEnableTopics\":true,\"wgGETopicsMatchModeEnabled\":false,\"wgGEStructuredTaskRejectionReasonTextInputEnabled\":false,\"wgGELevelingUpEnabledForUser\":false};RLSTATE={\"skins.vector.user.styles\":\"ready\",\"ext.globalCssJs.user.styles\":\"ready\",\"site.styles\":\"ready\",\"user.styles\":\"ready\",\"skins.vector.user\":\"ready\",\"ext.globalCssJs.user\":\"ready\",\"user\":\"ready\",\"user.options\":\"loading\",\"ext.cite.styles\":\"ready\",\"ext.math.styles\":\"ready\",\"codex-search-styles\":\"ready\",\"skins.vector.styles\":\"ready\",\"skins.vector.icons\":\"ready\",\"jquery.makeCollapsible.styles\":\"ready\",\"ext.visualEditor.desktopArticleTarget.noscript\":\"ready\",\"ext.uls.interlanguage\":\"ready\",\"wikibase.client.init\":\"ready\",\"ext.wikimediaBadges\":\"ready\"};RLPAGEMODULES=[\"ext.cite.ux-enhancements\",\"mediawiki.page.media\",\"site\",\"mediawiki.page.ready\",\"jquery.makeCollapsible\",\"mediawiki.toc\",\"skins.vector.js\",\"ext.centralNotice.geoIP\",\"ext.centralNotice.startUp\",\"ext.gadget.ReferenceTooltips\",\"ext.gadget.switcher\",\n",
            "\"ext.urlShortener.toolbar\",\"ext.centralauth.centralautologin\",\"mmv.head\",\"mmv.bootstrap.autostart\",\"ext.popups\",\"ext.visualEditor.desktopArticleTarget.init\",\"ext.visualEditor.targetLoader\",\"ext.echo.centralauth\",\"ext.eventLogging\",\"ext.wikimediaEvents\",\"ext.navigationTiming\",\"ext.uls.interface\",\"ext.cx.eventlogging.campaigns\",\"ext.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cleaning data from symbols or characters\n",
        "cleaned_text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "# Remove single characters\n",
        "cleaned_text = re.sub(r'\\b\\w\\b', '', cleaned_text)"
      ],
      "metadata": {
        "id": "a296zBMLJEsX"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MrSQLA0GHvor",
        "outputId": "ca2f88fc-3317-449a-9217-72ec5799f38a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique Words:\n",
            "{'similar', 'nileftv_w_icdot', 'next', 'described', 'ev_wcdot', 'memory', '8869', 'doc2vecedit', 'major', 'genomics', 'poorly', 'patterns18', 'found', 'input', 'used', 'cjin', 'leverage', 'common', 'mofrad18', 'superior', 'report', 'dna', 'general', 'extraction', 'wordneighbor', 'new', 'task', 'pvdm', 'depending', 'weighs', 'approximation', 'party', '201323', 'architecture', 'threshold', 'hand', 'understood', 'every', 'subsamplingedit', 'identified', 'per', 'recurrent', 'work', 'determines', 'canonical', 'historyedit', 'pvdbow', 'information', 'left', 'usage', 'context', 'entire', 'external', 'bag', 'word11', 'reaching', 'independently', 'stop', 'recommended', 'version', 'assessing', 'gradient', 'large', 'order', 'vsum', 'vev_wcdot', 'representation', '100', 'provide', 'neuralnetwork', 'influence', 'infrequent', 'v_w_jright', 'underlying', 'machine', 'partial', 'minimizing', 'involves', 'semantically', 'vectors19', 'performing', 'way', 'appearing', 'good', 'logarithm', 'doe', 'colleague', 'python', 'softmax', 'highlighted', 'meaningful', 'words24', 'led', 'successfully', 'compared', 'distant', 'relation', 'supporting', 'able', 'highdimension', 'capture', 'tree', 'topic15', 'problem', 'synonym', 'would', 'unstructured', 'still', 'benchmark', 'woman', 'model', 'needlemanwunsch', 'biophysical', 'gene', 'iwe', 'v_w_jln', 'computing', 'respectively', 'loglikelihood', 'uk13', 'far', 'make', 'share', 'content', 'reduces', 'size26', 'ascertain', 'sliding', 'hierarchical', 'topic', 'analogy', 'detect', 'search', 'skipgram', 'synonymous', 'use', 'vectorsum', 'javascala', 'berlin', 'utilize', 'distributed', 'diminishes1', 'closeness', 'refer', 'word', 'style', 'utilizes', 'increased', 'coauthor', 'step', 'hundred', 'according', 'attention', 'proposed', 'medicine', 'product', 'aminoacid', 'advantage', 'outofvocabulary', 'greatly', 'numerically', 'arithmetic', 'seen', 'andor', 'expensive', 'set', 'preservation', 'text', 'test', 'continuously', 'goldberg', 'method', 'produce', 'transferring', 'write', 'obtaining', 'expositions45', 'training', 'including', 'arguing', 'tackle', 'outperforming', 'twolayer', 'generate', 'se', 'al1', 'measured', 'unique', 'narrative', 'note', 'level', 'learned', 'want', 'computational', 'helped', 'accuracy', 'parametrization', 'doubling', 'combine', 'idea', 'neighbor', 'closest', 'loglinear', 'objective', 'outofsample', 'come', 'across', 'facet', 'dated', 'successful', 'using', 'congress', 'similarity', 'wa', 'ni', 'application', 'top2vecedit', 'team', 'useful9', 'depends', 'processing', 'variablelength', 'variation', 'affect', 'cause', '201521', 'property', 'documents1112', 'identifier', 'handwavy', 'little', 'removed', 'otherwise', 'mechanism', 'forced', '_iin', 'represents', 'point', 'ungrammatical', 'named', 'yield', 'appearance', 'unrelated', 'formal', 'relationshipsedit', 'equation', 'regarded', 'university', 'high', 'vectors3', 'additionally', '2022', 'mapping', 'low', 'analyse', 'heavily', 'cluster', 'syntactically', 'therefore', 'medium', 'random', 'learn', 'certain', 'variant', 'le', 'ha', 'finally', 'calculation', 'w_jjin', 'suggest', 'important', 'several', 'detailsedit', 'discussed', 'lda', 'rediright', 'downstream', 'ambiguity', 'n43211234', 'negative', 'whereas', 'much', 'epoch', 'case', 'considered', 'subtopics', 'slow', 'hypothesis', 'shallow', 'dimensional', 'v_w_color', 'continuous', 'sum', 'latent', 'lower', 'win', 'lexical', 'offer', 'farther', 'original', 'cbowedit', 'one', 'published', 'developed', 'value', 'distributional', 'span', 'taking', 'dotproductsoftmax', 'additional', 'computationally', 'algorithm4', 'scanned', '201622', 'around', 'related', 'lowfrequency', 'allows', 'approach', 'quality', 'first', 'consistently', 'prw_jw_i', 'translated', '15', 'data', 'skipgramedit', 'intelligent', 'dimensionality', 'vocabulary', 'reduce', 'included', 'let', 'reconstruct', 'centroid', 'blank', '2010', 'unstable', 'displaystyle', 'hierarchy', 'assist', 'radiology', 'algorithms1further', 'implemented', 'qualityedit', '20131', 'nlp8', 'assumption', 'gain', 'hyperparameters', 'word2vec', 'phase', 'identical', 'eg', 'generalizability', 'maximize', 'group', 'doc2vec', 'opposed', 'infrequently', 'explanation', 'occur', 'tense', 'neural', 'rna', 'art', 'draw', 'generally', 'measure', 'operation', 'must', 'biggest', 'generated', 'applied', 'cprw_jjin', 'different', 'earlier', 'position', 'predicted', 'linguistic', '50', 'genevec', 'algebraic', 'embedding', 'simply', 'however', 'furthermore', 'number', 'single', 'based', 'ln', 'away', 'embeddings', 'sample', 'amount', 'showed', 'java', 'modeledit', 'issue', 'period', 'preferred', 'allegory', 'extensionsedit', 'conditional', 'compute', 'nlp', 'niv_w_j', 'germany', 'challenge', 'dna2vec', 'near', 'free', 'constructed', 'cbow3', 'tensepast', 'semantic', 'studied', 'trick', 'topics1516', 'plausible1', 'domain', 'marginal', 'acronym', 'niw_iprod', 'wordembedding', 'often', 'umap', 'argue', 'note3', 'higher', 'provides', 'piece', 'seems', 'niw_i', 'difference', 'creating', 'handle', 'space1', 'unknown', 'close', 'metric', 'sampling', 'analysisedit', 'language', 'asgari', 'cost', 'niprw_jw_i', 'papers12', 'implementationsedit', 'logprobability', 'dissimilar', '10', 'framework', 'brother', 'dictionary', 'specific', 'map', 'et', 'proteinvectors', 'institutional', 'except', 'algorithm', 'alsoedit', 'straight', 'parameter', 'biochemical', 'tool', 'line', 'bioinformatics', 'window', 'radiologist', 'best', 'current', 'following', 'add', 'parameterizationedit', 'biovectors', 'proteomics', 'complexity', 'time1', 'correlation', 'appear', 'sister', 'viewed', 'huffman', 'probability', 'improvement', 'choice', 'increase', 'follows', 'improved', 'red', 'may', 'overall', 'altszyler', 'mikolov', 'top2vec', 'predict', 'obtain', 'two', '2013', 'generates', 'score', 'ideal', 'time', 'biovectorsedit', 'suppose', 'traditional', 'mapped', 'ran', 'biological', 'network', 'inferring', 'degree', 'institutions14', 'word2vec25', 'multiple', 'inference', 'linksedit', 'learning', '_jin', 'requires', 'thus', 'needed', 'prww_jjin', 'instead', 'might', 'surrounding', 'mathematical', 'interest', 'dense', 'either', 'particular', 'title', 'equivalent', 'windowedit', 'generative', 'function', 'performance', 'prod', 'words12', 'right', 'intractable', 'cbow', 'prediction', 'highfrequency', 'million', 'simplification', 'sampled', 'distance', 'demonstrates', 'present', 'translation', 'dimension', 'small', 'user', 'increasing', 'document', 'typically', 'maximized', 'results1', 'individual', 'positioned', 'abbreviation', 'result', 'protein', 'preferable4', 'upon', 'yielding', 'quantity', '10675', 'sensitive', 'well', 'second', 'paragraph', 'modelling6', 'subsampled', 'banerjee', 'solve', 'another', 'nifrac', 'numerical', 'indicate', 'summing', 'see', 'approachedit', 'niln', 'analysis', 'infer', 'corpus', 'state', 'curve', 'reproduced', 'process', 'ability', 'biovec', 'exploited', 'publication', 'arbitrary', 'attempt', 'unseen', 'ordering', 'size', 'hdbscan', 'include', 'corresponding', 'cosine', 'sequence', 'shown', 'develop', 'bagofwords', 'instance', 'relationship', 'space', 'hidden', 'interpretation', 'created', 'speaker', 'located', 'given', 'transformer', 'prw_jjin', 'telegraphic', 'protvec', 'institution', 'ascent', 'also', 'vector', 'syntactic', 'explain', 'considers', 'author', 'mathbb', 'levy', 'better', 'cln', 'characterize', 'arora', 'prw_iw_jjin', 'google', 'seek', 'highest', 'particularly', 'steep', 'technology', 'al', 'walk', 'fill', 'trained', 'attribute', 'reason', 'encountered', 'relative', 'cprw_iw_jjin', 'prompted', 'setting', 'job', 'estimate', 'simple', 'clinical', 'together', 'morphologically', 'various', 'nevertheless', 'range', 'political', 'extension', 'governmental', 'approximate', 'show', '_win', 'algorithmedit', 'training10', 'variety', 'maximization', 'frequent', 'researcher', 'lsa', 'us', 'goal', 'hdbscan17', 'faster', 'al20', 'convert', 'parliament', 'intuitively', 'iterates', 'brno', 'dimensionalityedit', 'section', 'example', 'oov', 'total', 'dataset', 'generation', 'sentence', 'tomáš', 'pattern', 'modeling', 'layer', 'natural', 'iweedit', 'countrycapital', 'referencesedit', '2017', 'assigned', 'term', 'fast', 'firth', 'technique', 'v_win', 'nearby', 'frequency', 'many', 'like', 'widely', 'man', 'speed', '1000', 'genevectors', 'ngrams', 'meaning', 'top', 'patented7', 'proceeds', 'u', 'take', 'challenging'}\n"
          ]
        }
      ],
      "source": [
        "# Normalization\n",
        "normalized_text = cleaned_text.lower()\n",
        "\n",
        "# Tokenization\n",
        "tokens = word_tokenize(normalized_text)\n",
        "\n",
        "# Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "#remove stop words\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "filtered_tokens = [token for token in lemmatized_tokens if token not in stop_words]\n",
        "\n",
        "#git Unique Words\n",
        "unique_words = set(filtered_tokens)\n",
        "\n",
        "print(\"Unique Words:\")\n",
        "print(unique_words)\n"
      ]
    }
  ]
}